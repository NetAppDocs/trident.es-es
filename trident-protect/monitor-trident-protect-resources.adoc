---
permalink: trident-protect/monitor-trident-protect-resources.html 
sidebar: sidebar 
keywords: manage, authentication, rbac 
summary: Puede supervisar el estado de Trident Protect Resources usando métricas de estado-kube y Prometheus. Esto proporciona información de estado sobre implementaciones, nodos y pods. 
---
= Supervise los recursos de Trident Protect
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Puede utilizar las herramientas de código abierto kube-state-metrics, Prometheus y Alertmanager para supervisar el estado de los recursos protegidos por Trident Protect.

El servicio kube-state-metrics genera métricas a partir de la comunicación de la API de Kubernetes. El uso de la tecnología con Trident Protect proporciona información útil sobre el estado de los recursos de su entorno.

Prometheus es un kit de herramientas que puede ingerir los datos generados por kube-state-metrics y presentarlos como información fácilmente legible sobre estos objetos. Juntos, las métricas de estado-kube y Prometheus proporcionan una forma de supervisar el estado y el estado de los recursos que gestiona con Trident Protect.

Alertmanager es un servicio que ingiere las alertas enviadas por herramientas como Prometheus y las redirige a los destinos que configure.

[NOTE]
====
Las configuraciones y directrices incluidas en estos pasos son solo ejemplos; debe personalizarlas para que se adapten a su entorno. Consulte la siguiente documentación oficial para obtener instrucciones específicas y soporte:

* https://github.com/kubernetes/kube-state-metrics/tree/main["documentación sobre métricas del estado de kube"^]
* https://prometheus.io/docs/introduction/overview/["Documentación de Prometheus"^]
* https://github.com/prometheus/alertmanager["Documentación de Alertmanager"^]


====


== Paso 1: Instale las herramientas de monitoreo

Para habilitar la supervisión de recursos en Trident Protect, debe instalar y configurar kube-state-metrics, Promethus y Alertmanager.



=== Instale kube-state-metrics

Puede instalar kube-state-metrics usando Helm.

.Pasos
. Agregue el gráfico Helm de métricas de estado-kube. Por ejemplo:
+
[source, console]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
----
. Cree un archivo de configuración para el diagrama Helm (por ejemplo, `metrics-config.yaml`). Puede personalizar la siguiente configuración de ejemplo para que coincida con su entorno:
+
.Metrics-config.yaml: Configuración del gráfico Helm de métricas de estado de kube
[source, yaml]
----
---
extraArgs:
  # Collect only custom metrics
  - --custom-resource-state-only=true

customResourceState:
  enabled: true
  config:
    kind: CustomResourceStateMetrics
    spec:
      resources:
      - groupVersionKind:
          group: protect.trident.netapp.io
          kind: "Backup"
          version: "v1"
        labelsFromPath:
          backup_uid: [metadata, uid]
          backup_name: [metadata, name]
          creation_time: [metadata, creationTimestamp]
        metrics:
        - name: backup_info
          help: "Exposes details about the Backup state"
          each:
            type: Info
            info:
              labelsFromPath:
                appVaultReference: ["spec", "appVaultRef"]
                appReference: ["spec", "applicationRef"]
rbac:
  extraRules:
  - apiGroups: ["protect.trident.netapp.io"]
    resources: ["backups"]
    verbs: ["list", "watch"]

# Collect metrics from all namespaces
namespaces: ""

# Ensure that the metrics are collected by Prometheus
prometheus:
  monitor:
    enabled: true
----
. Instale kube-state-metrics mediante el despliegue del gráfico Helm. Por ejemplo:
+
[source, console]
----
helm install custom-resource -f metrics-config.yaml prometheus-community/kube-state-metrics --version 5.21.0
----
. Configure kube-state-metrics para generar métricas para los recursos personalizados utilizados por Trident Protect siguiendo las instrucciones de https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md#custom-resource-state-metrics["documentación de recursos personalizados de kube-state-metrics"^] .




=== Instale Prometheus

Puede instalar Prometheus siguiendo las instrucciones del https://prometheus.io/docs/prometheus/latest/installation/["Documentación de Prometheus"^] .



=== Instale Alertmanager

Puede instalar Alertmanager siguiendo las instrucciones del https://github.com/prometheus/alertmanager?tab=readme-ov-file#install["Documentación de Alertmanager"^] .



== Paso 2: Configure las herramientas de monitoreo para que funcionen juntas

Después de instalar las herramientas de supervisión, debe configurarlas para que funcionen juntas.

.Pasos
. Integre métricas de estado-kube con Prometheus. Edite el archivo de configuración Prometheus (`prometheus.yaml`) y agregue la información del servicio kube-state-metrics. Por ejemplo:
+
.prometheus.yaml: integración del servicio kube-state-metrics con Prometheus
[source, yaml]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: trident-protect
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.trident-protect.svc:8080']
----
. Configurar Prometheus para enrutar alertas a Alertmanager. Edite el archivo de configuración de Prometheus (`prometheus.yaml`) y agregue la siguiente sección:
+
.prometheus.yaml: Enviar alertas a Alertmanager
[source, yaml]
----
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager.trident-protect.svc:9093
----


.Resultado
Ahora Prometheus puede recopilar métricas de kube-state-metrics y puede enviar alertas a Alertmanager. Ahora está listo para configurar qué condiciones desencadenan una alerta y dónde se deben enviar las alertas.



== Paso 3: Configure las alertas y los destinos de alertas

Después de configurar las herramientas para que funcionen juntas, debe configurar qué tipo de información activa alertas y dónde se deben enviar las alertas.



=== Ejemplo de alerta: Fallo de backup

En el siguiente ejemplo se define una alerta crucial que se activa cuando el estado del recurso personalizado de backup se establece en `Error` 5 segundos o más. Puede personalizar este ejemplo para que coincida con su entorno e incluir este fragmento de YAML en su `prometheus.yaml` archivo de configuración:

.rules.yaml: Define una alerta de Prometheus para copias de seguridad fallidas
[source, yaml]
----
rules.yaml: |
  groups:
    - name: fail-backup
        rules:
          - alert: BackupFailed
            expr: kube_customresource_backup_info{status="Error"}
            for: 5s
            labels:
              severity: critical
            annotations:
              summary: "Backup failed"
              description: "A backup has failed."
----


=== Configure Alertmanager para que envíe alertas a otros canales

Puede configurar Alertmanager para que envíe notificaciones a otros canales, como correo electrónico, PagerDuty, Microsoft Teams u otros servicios de notificación especificando la configuración respectiva en `alertmanager.yaml` el archivo.

El siguiente ejemplo configura Alertmanager para enviar notificaciones a un canal de Slack. Para personalizar este ejemplo a su entorno, reemplace el valor de `api_url` la clave por la URL del webhook de Slack utilizada en su entorno:

.alertmanager.yaml: envía alertas a un canal de Slack
[source, yaml]
----
data:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      receiver: 'slack-notifications'
    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - api_url: '<your-slack-webhook-url>'
            channel: '#failed-backups-channel'
            send_resolved: false
----